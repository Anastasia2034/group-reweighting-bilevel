{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import the weight_searcher object\n",
    "from optweights.weight_searcher import WeightSearcher\n",
    "\n",
    "# import the logistic regression model from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# import numpy\n",
    "import numpy as np\n",
    "\n",
    "# create some arbitrary data\n",
    "n, d, k = 2000, 100, 2\n",
    "X, y = make_classification(\n",
    "    n_samples=n,\n",
    "    n_features=d,\n",
    "    n_classes=k,\n",
    "    random_state=42,\n",
    ")\n",
    "g = np.random.binomial(1, 0.5, size=n) + 1\n",
    "y, g = y.reshape(-1, 1), g.reshape(-1, 1)\n",
    "\n",
    "# make a train/validation split for the data\n",
    "n_train = int(n * 0.8)\n",
    "X_train, y_train, g_train = X[:n_train], y[:n_train], g[:n_train]\n",
    "X_val, y_val, g_val = X[n_train:], y[n_train:], g[n_train:]\n",
    "\n",
    "# create a logistic regression model\n",
    "model_param  = {'max_iter': 100,\n",
    "                'penalty': 'l1',\n",
    "                'C': 1,\n",
    "                'solver': 'liblinear',\n",
    "                'tol': 1e-4,\n",
    "                'verbose': 0,\n",
    "                'random_state': 0,\n",
    "                'fit_intercept': True, \n",
    "                'warm_start': False}\n",
    "logreg = LogisticRegression(**model_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anastasiaanastasiadou/Downloads/base-optweights-main/.env/lib/python3.11/site-packages/sklearn/utils/validation.py:1406: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train a g classifier\n",
    "g_classifier = LogisticRegression(**model_param)\n",
    "g_classifier.fit(X_train, g_train)\n",
    "g_prob = g_classifier.predict_proba(X_train)\n",
    "\n",
    "# I assume the first column is g=1, second g=2, etc. \n",
    "print(g_prob.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 0, the loss is 0.2977, we have 5 patience left, and the probabilities are {1: 0.5, 2: 0.5}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6957340240478516 seconds\n",
      "At step 1, the loss is 0.2977, we have 5 patience left, and the probabilities are {1: 0.5001, 2: 0.4999}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6888651847839355 seconds\n",
      "At step 2, the loss is 0.2977, we have 5 patience left, and the probabilities are {1: 0.5003, 2: 0.4997}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6757698059082031 seconds\n",
      "At step 3, the loss is 0.2977, we have 5 patience left, and the probabilities are {1: 0.5005, 2: 0.4995}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6780920028686523 seconds\n",
      "At step 4, the loss is 0.2977, we have 5 patience left, and the probabilities are {1: 0.5007, 2: 0.4993}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6841809749603271 seconds\n",
      "At step 5, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5009, 2: 0.4991}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6874120235443115 seconds\n",
      "At step 6, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5011, 2: 0.4989}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6729202270507812 seconds\n",
      "At step 7, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5013, 2: 0.4987}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.691431999206543 seconds\n",
      "At step 8, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5015, 2: 0.4985}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6830480098724365 seconds\n",
      "At step 9, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5017, 2: 0.4983}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.683786153793335 seconds\n",
      "At step 10, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5019, 2: 0.4981}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.684190034866333 seconds\n",
      "At step 11, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5021, 2: 0.4979}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6871838569641113 seconds\n",
      "At step 12, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5023, 2: 0.4977}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6837151050567627 seconds\n",
      "At step 13, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5025, 2: 0.4975}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6881368160247803 seconds\n",
      "At step 14, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5027, 2: 0.4973}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6825182437896729 seconds\n",
      "At step 15, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5029, 2: 0.4971}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6786317825317383 seconds\n",
      "At step 16, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5031, 2: 0.4969}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.678685188293457 seconds\n",
      "At step 17, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5033, 2: 0.4967}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6906819343566895 seconds\n",
      "At step 18, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5035, 2: 0.4965}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.736839771270752 seconds\n",
      "At step 19, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5037, 2: 0.4963}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6927499771118164 seconds\n",
      "At step 20, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5039, 2: 0.4961}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6819877624511719 seconds\n",
      "At step 21, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5041, 2: 0.4959}, which sum to 1.0000 with gradients {1: -0.0037, 2: 0.0037}.\n",
      "The model is updated in 0.6869606971740723 seconds\n",
      "At step 22, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5043, 2: 0.4957}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.6807360649108887 seconds\n",
      "At step 23, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5045, 2: 0.4955}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.6814827919006348 seconds\n",
      "At step 24, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5047, 2: 0.4953}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.6812689304351807 seconds\n",
      "At step 25, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5049, 2: 0.4951}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.6811792850494385 seconds\n",
      "At step 26, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5051, 2: 0.4949}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.771507978439331 seconds\n",
      "At step 27, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5053, 2: 0.4947}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7718501091003418 seconds\n",
      "At step 28, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5055, 2: 0.4945}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7712531089782715 seconds\n",
      "At step 29, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5057, 2: 0.4943}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7717809677124023 seconds\n",
      "At step 30, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5059, 2: 0.4941}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7714550495147705 seconds\n",
      "At step 31, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5061, 2: 0.4939}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7797560691833496 seconds\n",
      "At step 32, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5063, 2: 0.4937}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7694602012634277 seconds\n",
      "At step 33, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5065, 2: 0.4935}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7672939300537109 seconds\n",
      "At step 34, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5067, 2: 0.4933}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7673611640930176 seconds\n",
      "At step 35, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5069, 2: 0.4931}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.773414134979248 seconds\n",
      "At step 36, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5071, 2: 0.4929}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.775562047958374 seconds\n",
      "At step 37, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5073, 2: 0.4927}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7907001972198486 seconds\n",
      "At step 38, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5075, 2: 0.4925}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7939298152923584 seconds\n",
      "At step 39, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5077, 2: 0.4923}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7767601013183594 seconds\n",
      "At step 40, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5079, 2: 0.4921}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7781751155853271 seconds\n",
      "At step 41, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5081, 2: 0.4919}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7779598236083984 seconds\n",
      "At step 42, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5083, 2: 0.4917}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.778770923614502 seconds\n",
      "At step 43, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5085, 2: 0.4915}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7781822681427002 seconds\n",
      "At step 44, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5087, 2: 0.4913}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7774488925933838 seconds\n",
      "At step 45, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5089, 2: 0.4911}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7814590930938721 seconds\n",
      "At step 46, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5091, 2: 0.4909}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7806868553161621 seconds\n",
      "At step 47, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5093, 2: 0.4907}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7783212661743164 seconds\n",
      "At step 48, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5095, 2: 0.4905}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.776587963104248 seconds\n",
      "At step 49, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5097, 2: 0.4903}, which sum to 1.0000 with gradients {1: -0.0036, 2: 0.0036}.\n",
      "The model is updated in 0.7773239612579346 seconds\n",
      "At step 50, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5099, 2: 0.4901}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.7783088684082031 seconds\n",
      "At step 51, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5101, 2: 0.4899}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.778188943862915 seconds\n",
      "At step 52, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5103, 2: 0.4897}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.8193960189819336 seconds\n",
      "At step 53, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5105, 2: 0.4895}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.7783958911895752 seconds\n",
      "At step 54, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5107, 2: 0.4893}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.7777130603790283 seconds\n",
      "At step 55, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5109, 2: 0.4891}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.7775919437408447 seconds\n",
      "At step 56, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5111, 2: 0.4889}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.7788848876953125 seconds\n",
      "At step 57, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5113, 2: 0.4887}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.7791159152984619 seconds\n",
      "At step 58, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5115, 2: 0.4885}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.7790899276733398 seconds\n",
      "At step 59, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5117, 2: 0.4883}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.7777280807495117 seconds\n",
      "At step 60, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5119, 2: 0.4881}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.8150138854980469 seconds\n",
      "At step 61, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5121, 2: 0.4879}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.8169517517089844 seconds\n",
      "At step 62, the loss is 0.2976, we have 5 patience left, and the probabilities are {1: 0.5123, 2: 0.4877}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.8167948722839355 seconds\n",
      "At step 63, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5125, 2: 0.4875}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.8220486640930176 seconds\n",
      "At step 64, the loss is 0.2975, we have 4 patience left, and the probabilities are {1: 0.5127, 2: 0.4873}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.8152778148651123 seconds\n",
      "At step 65, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5129, 2: 0.4871}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.8152480125427246 seconds\n",
      "At step 66, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5131, 2: 0.4869}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.8176889419555664 seconds\n",
      "At step 67, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5133, 2: 0.4867}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.8173229694366455 seconds\n",
      "At step 68, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5135, 2: 0.4865}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.8173398971557617 seconds\n",
      "At step 69, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5137, 2: 0.4863}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.7781920433044434 seconds\n",
      "At step 70, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5139, 2: 0.4861}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.7819490432739258 seconds\n",
      "At step 71, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5141, 2: 0.4859}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.7753019332885742 seconds\n",
      "At step 72, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5143, 2: 0.4857}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.7671868801116943 seconds\n",
      "At step 73, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5145, 2: 0.4855}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.7666699886322021 seconds\n",
      "At step 74, the loss is 0.2975, we have 4 patience left, and the probabilities are {1: 0.5147, 2: 0.4853}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.7727758884429932 seconds\n",
      "At step 75, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5149, 2: 0.4851}, which sum to 1.0000 with gradients {1: -0.0035, 2: 0.0035}.\n",
      "The model is updated in 0.818803071975708 seconds\n",
      "At step 76, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5151, 2: 0.4849}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.8118469715118408 seconds\n",
      "At step 77, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5153, 2: 0.4847}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.8008289337158203 seconds\n",
      "At step 78, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5155, 2: 0.4845}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.8073060512542725 seconds\n",
      "At step 79, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5157, 2: 0.4843}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.8078742027282715 seconds\n",
      "At step 80, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5159, 2: 0.4841}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.8107259273529053 seconds\n",
      "At step 81, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5161, 2: 0.4839}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.8085010051727295 seconds\n",
      "At step 82, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5163, 2: 0.4837}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.863577127456665 seconds\n",
      "At step 83, the loss is 0.2975, we have 4 patience left, and the probabilities are {1: 0.5165, 2: 0.4835}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.8057441711425781 seconds\n",
      "At step 84, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5167, 2: 0.4833}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.7890810966491699 seconds\n",
      "At step 85, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5169, 2: 0.4831}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.8022129535675049 seconds\n",
      "At step 86, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5171, 2: 0.4829}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.8040211200714111 seconds\n",
      "At step 87, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5173, 2: 0.4827}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.7980690002441406 seconds\n",
      "At step 88, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5175, 2: 0.4825}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.7935559749603271 seconds\n",
      "At step 89, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5177, 2: 0.4823}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.7983250617980957 seconds\n",
      "At step 90, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5179, 2: 0.4821}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.7942779064178467 seconds\n",
      "At step 91, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5181, 2: 0.4819}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.7917261123657227 seconds\n",
      "At step 92, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5183, 2: 0.4817}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.7793569564819336 seconds\n",
      "At step 93, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5185, 2: 0.4815}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.7886679172515869 seconds\n",
      "At step 94, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5187, 2: 0.4813}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.7883338928222656 seconds\n",
      "At step 95, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5189, 2: 0.4811}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.7881350517272949 seconds\n",
      "At step 96, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5191, 2: 0.4809}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.7833070755004883 seconds\n",
      "At step 97, the loss is 0.2975, we have 4 patience left, and the probabilities are {1: 0.5193, 2: 0.4807}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.7821109294891357 seconds\n",
      "At step 98, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5195, 2: 0.4805}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.7825760841369629 seconds\n",
      "At step 99, the loss is 0.2975, we have 5 patience left, and the probabilities are {1: 0.5197, 2: 0.4803}, which sum to 1.0000 with gradients {1: -0.0034, 2: 0.0034}.\n",
      "The model is updated in 0.7824647426605225 seconds\n",
      "Returning the p={1: 0.5196558967690307, 2: 0.48034410323096927}, for which loss is 0.29749981756383803\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the probability of each group in the distribution of interest\n",
    "# This is a case where we have two groups, and each group is given equal weight\n",
    "p_ood = {1: 0.5, 2: 0.5}\n",
    "\n",
    "# create a weight searcher object\n",
    "ws = WeightSearcher(X_train, y_train, g_train, X_val, y_val, g_val, # define the X, y, g for both train/val\n",
    "                        p_ood=p_ood,                                 # define the distribution of interest\n",
    "                        sklearn_model=logreg,                        # define the sklearn model (optional)\n",
    "                     )\n",
    "\n",
    "# set the g_prob\n",
    "ws.set_g_prob(g_prob)\n",
    "\n",
    "# define the arguments for the optimization\n",
    "T = 100             # the number of steps\n",
    "lr = 0.1            # the learning rate\n",
    "momentum = 0.5      # the momentum parameter - higher is more momentum\n",
    "patience=5\n",
    "\n",
    "# optimize the weights\n",
    "p_hat =  ws.optimize_weights(T,  lr,  momentum, patience=patience)\n",
    "\n",
    "# get the weights for the training set - these can then be used subsequently for an estimator. \n",
    "w_train = ws.return_weights(p_hat, g_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Re-define data generation functions\n",
    "def generate_subset(n: int, p: float, beta: float,\n",
    "                   sigma_1: float, sigma_0: float, mu, gamma,\n",
    "                   a_0: float, a_1: float, d: int):\n",
    "    Sigma = np.diag(np.ones(d)) * gamma\n",
    "    X = np.random.multivariate_normal(mu, Sigma, n)\n",
    "\n",
    "    g = np.zeros(n)\n",
    "    n_1 = int(np.round(n * p))\n",
    "    n_0 = n - n_1\n",
    "    i_1 = np.random.choice(n, n_1, replace=False)\n",
    "    g[i_1] = 1\n",
    "\n",
    "    y = np.zeros(n)\n",
    "\n",
    "    X_1 = X[g == 1]\n",
    "    eps_1 = np.random.normal(0, np.sqrt(sigma_1), n_1).reshape(-1, 1)\n",
    "    B_1 = np.zeros((d, 1))\n",
    "    B_1[0] = beta\n",
    "    y_1 = np.matmul(X_1, B_1) + a_1 + eps_1\n",
    "    y[g == 1] = y_1.squeeze(-1)\n",
    "\n",
    "    X_0 = X[g == 0]\n",
    "    eps_0 = np.random.normal(0, np.sqrt(sigma_0), n_0).reshape(-1, 1)\n",
    "    B_0 = np.zeros((d, 1))\n",
    "    B_0[0] = beta\n",
    "    y_0 = np.matmul(X_0, B_0) + a_0 + eps_0\n",
    "    y[g == 0] = y_0.squeeze(-1)\n",
    "\n",
    "    y = y.reshape(-1, 1)\n",
    "    g = g.reshape(-1, 1)\n",
    "\n",
    "    return X, y, g\n",
    "\n",
    "def generate_toy_dataset(n: int, p_tr: float, p_te: float, beta: float,\n",
    "                         sigma_1: float, sigma_0: float, intercept_diff: float, d: int):\n",
    "    mu = np.zeros(d)\n",
    "    gamma = 1.0\n",
    "    a_0 = 0.0\n",
    "    a_1 = a_0 + intercept_diff\n",
    "\n",
    "    n_train = int(0.6 * n)\n",
    "    n_val = int(0.2 * n)\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    X_train, y_train, g_train = generate_subset(n_train, p_tr, beta, sigma_1, sigma_0, mu, gamma, a_0, a_1, d)\n",
    "    X_val, y_val, g_val = generate_subset(n_val, p_tr, beta, sigma_1, sigma_0, mu, gamma, a_0, a_1, d)\n",
    "    X_test, y_test, g_test = generate_subset(n_test, p_te, beta, sigma_1, sigma_0, mu, gamma, a_0, a_1, d)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, g_train, g_val, g_test\n",
    "\n",
    "# Run the simulation framework\n",
    "def run_simulation(n=1000, d=5, p_tr=0.7, p_te=0.3, beta=1.0, intercept_diff=1.0,\n",
    "                   sigma_0=1.0, sigma_1=1.0, n_trials=50, reweight=False):\n",
    "    mse_list = []\n",
    "\n",
    "    for seed in range(n_trials):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        data = generate_toy_dataset(n=n, p_tr=p_tr, p_te=p_te, beta=beta,\n",
    "                                     sigma_0=sigma_0, sigma_1=sigma_1,\n",
    "                                     intercept_diff=intercept_diff, d=d)\n",
    "\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, g_train, g_val, g_test = data\n",
    "\n",
    "        X_train_aug = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "        X_test_aug = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "\n",
    "        if reweight:\n",
    "            p_train_1 = np.mean(g_train)\n",
    "            p_test_1 = np.mean(g_test)\n",
    "            p_train_0 = 1 - p_train_1\n",
    "            p_test_0 = 1 - p_test_1\n",
    "            w = np.where(g_train.flatten() == 1, p_test_1 / p_train_1, p_test_0 / p_train_0)\n",
    "        else:\n",
    "            w = None\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_aug, y_train, sample_weight=w)\n",
    "\n",
    "        y_pred = model.predict(X_test_aug)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_list.append(mse)\n",
    "\n",
    "    return np.mean(mse_list), np.std(mse_list)\n",
    "\n",
    "# Execute and compare unweighted vs weighted results\n",
    "mean_mse_unweighted, std_mse_unweighted = run_simulation(reweight=False)\n",
    "mean_mse_weighted, std_mse_weighted = run_simulation(reweight=True)\n",
    "\n",
    "# Plot\n",
    "labels = ['Unweighted', 'Reweighted']\n",
    "means = [mean_mse_unweighted, mean_mse_weighted]\n",
    "errors = [std_mse_unweighted, std_mse_weighted]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, means, yerr=errors, capsize=5)\n",
    "plt.ylabel('Test MSE')\n",
    "plt.title('Effect of Reweighting on Test Error')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define the simplex projection function for weight updates\n",
    "def project_to_simplex(p):\n",
    "    \"\"\" Project weights to the probability simplex using sorting-based algorithm \"\"\"\n",
    "    p = np.maximum(p, 0)\n",
    "    u = np.sort(p)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    rho = np.where(u > (cssv - 1) / (np.arange(len(p)) + 1))[0][-1]\n",
    "    theta = (cssv[rho] - 1) / (rho + 1)\n",
    "    return np.maximum(p - theta, 0)\n",
    "\n",
    "# Define the optimizer loop for group weights\n",
    "def optimize_group_weights(weight_searcher, X_train, y_train, g_train,\n",
    "                           X_val, y_val, model, q_soft, p_train, p_init,\n",
    "                           n_steps=20, lr=0.1):\n",
    "    \"\"\"\n",
    "    Perform projected gradient descent to optimize group weights using IFT gradient.\n",
    "    \"\"\"\n",
    "    # Set soft group probabilities\n",
    "    weight_searcher.set_g_prob(q_soft)\n",
    "    \n",
    "    # Current weights (p)\n",
    "    p = p_init.copy()\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        # Update Weights object with current p\n",
    "        p_w = {g: p[g] for g in range(len(p))}\n",
    "        weight_searcher.reset_weights(p_w)\n",
    "\n",
    "        # Compute importance weights for validation (p_test / p_train)\n",
    "        p_val = {0: 0.5, 1: 0.5}  # assume balanced test groups\n",
    "        w_val = np.where(g_val.flatten() == 1,\n",
    "                         p_val[1] / p_train[1],\n",
    "                         p_val[0] / p_train[0])\n",
    "\n",
    "        # Compute gradient via IFT\n",
    "        grad = weight_searcher.weight_grad_via_ift(\n",
    "            X_train, y_train, g_train, X_val, y_val, model, w_val\n",
    "        )\n",
    "\n",
    "        # Update and project\n",
    "        p = p - lr * grad\n",
    "        p = project_to_simplex(p)\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_subset(n: int, p: float, beta: float, sigma_1: float, sigma_0: float, mu, gamma,\n",
    "                   a_0: float, a_1: float, d: int):\n",
    "    Sigma = np.diag(np.ones(d)) * gamma\n",
    "    X = np.random.multivariate_normal(mu, Sigma, n)\n",
    "    g = np.zeros(n)\n",
    "    n_1 = int(np.round(n * p))\n",
    "    i_1 = np.random.choice(n, n_1, replace=False)\n",
    "    g[i_1] = 1\n",
    "    y = np.zeros(n)\n",
    "\n",
    "    X_1 = X[g == 1]\n",
    "    eps_1 = np.random.normal(0, np.sqrt(sigma_1), n_1).reshape(-1, 1)\n",
    "    B_1 = np.zeros((d, 1))\n",
    "    B_1[0] = beta\n",
    "    y_1 = np.matmul(X_1, B_1) + a_1 + eps_1\n",
    "    y[g == 1] = y_1.squeeze(-1)\n",
    "\n",
    "    X_0 = X[g == 0]\n",
    "    eps_0 = np.random.normal(0, np.sqrt(sigma_0), n - n_1).reshape(-1, 1)\n",
    "    B_0 = np.zeros((d, 1))\n",
    "    B_0[0] = beta\n",
    "    y_0 = np.matmul(X_0, B_0) + a_0 + eps_0\n",
    "    y[g == 0] = y_0.squeeze(-1)\n",
    "\n",
    "    return X, y.reshape(-1, 1), g.reshape(-1, 1)\n",
    "\n",
    "def generate_toy_dataset(n, p_tr, p_te, beta, sigma_1, sigma_0, intercept_diff, d):\n",
    "    mu = np.zeros(d)\n",
    "    gamma = 1.0\n",
    "    a_0 = 0.0\n",
    "    a_1 = a_0 + intercept_diff\n",
    "    n_train, n_val = int(0.6 * n), int(0.2 * n)\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    X_train, y_train, g_train = generate_subset(n_train, p_tr, beta, sigma_1, sigma_0, mu, gamma, a_0, a_1, d)\n",
    "    X_val, y_val, g_val = generate_subset(n_val, p_tr, beta, sigma_1, sigma_0, mu, gamma, a_0, a_1, d)\n",
    "    X_test, y_test, g_test = generate_subset(n_test, p_te, beta, sigma_1, sigma_0, mu, gamma, a_0, a_1, d)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, g_train, g_val, g_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def run_simulation_with_consistent_group_labels(n=2000, d=10, p_tr=0.7, p_te=0.3, beta=1.0, intercept_diff=1.0,\n",
    "                                                 sigma_0=1.0, sigma_1=1.0, n_trials=5, T=100, lr=0.1,\n",
    "                                                 momentum=0.5, patience=5):\n",
    "    results = {'Unweighted': [], 'Analytical': [], 'Optimized': []}\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        np.random.seed(trial)\n",
    "        data = generate_toy_dataset(n=n, p_tr=p_tr, p_te=p_te, beta=beta,\n",
    "                                    sigma_0=sigma_0, sigma_1=sigma_1,\n",
    "                                    intercept_diff=intercept_diff, d=d)\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, g_train, g_val, g_test = data\n",
    "        g_train, g_val = g_train.astype(int) + 1, g_val.astype(int) + 1\n",
    "\n",
    "        median_threshold = np.median(y_train)\n",
    "        y_train_bin = (y_train > median_threshold).astype(int)\n",
    "        y_val_bin = (y_val > median_threshold).astype(int)\n",
    "        y_test_bin = (y_test > median_threshold).astype(int)\n",
    "\n",
    "        g_classifier = LogisticRegression(solver='liblinear')\n",
    "        g_classifier.fit(X_train, g_train.ravel())\n",
    "        g_prob = g_classifier.predict_proba(X_train)\n",
    "        class_order = g_classifier.classes_\n",
    "        g_to_idx = {g: i for i, g in enumerate(class_order)}\n",
    "        g_prob_ordered = np.zeros_like(g_prob)\n",
    "        for g, i in g_to_idx.items():\n",
    "            g_prob_ordered[:, g - 1] = g_prob[:, i]\n",
    "\n",
    "        p_train = {g: np.mean(g_train == g) for g in class_order}\n",
    "        p_test = {1: p_te, 2: 1 - p_te}\n",
    "\n",
    "        clf1 = LogisticRegression(solver='liblinear')\n",
    "        clf1.fit(X_train, y_train_bin.ravel())\n",
    "        acc1 = accuracy_score(y_test_bin, clf1.predict(X_test))\n",
    "        results['Unweighted'].append(acc1)\n",
    "\n",
    "        w_analytical = np.where(g_train.flatten() == 1,\n",
    "                                p_test[1] / p_train[1],\n",
    "                                p_test[2] / p_train[2])\n",
    "        clf2 = LogisticRegression(solver='liblinear')\n",
    "        clf2.fit(X_train, y_train_bin.ravel(), sample_weight=w_analytical)\n",
    "        acc2 = accuracy_score(y_test_bin, clf2.predict(X_test))\n",
    "        results['Analytical'].append(acc2)\n",
    "\n",
    "        from optweights.weight_searcher import WeightSearcher\n",
    "        ws = WeightSearcher(X_train, y_train_bin, g_train, X_val, y_val_bin, g_val,\n",
    "                            p_ood=p_test, sklearn_model=LogisticRegression(solver='liblinear'))\n",
    "        ws.set_g_prob(g_prob_ordered)\n",
    "        p_hat = ws.optimize_weights(T=T, lr=lr, momentum=momentum, patience=patience)\n",
    "        w_opt = ws.return_weights(p_hat, g_train)\n",
    "        clf3 = LogisticRegression(solver='liblinear')\n",
    "        clf3.fit(X_train, y_train_bin.ravel(), sample_weight=w_opt)\n",
    "        acc3 = accuracy_score(y_test_bin, clf3.predict(X_test))\n",
    "        results['Optimized'].append(acc3)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 0, the loss is 0.5367, we have 5 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0002, 2: -0.0002}.\n",
      "The model is updated in 0.0016183853149414062 seconds\n",
      "At step 1, the loss is 0.5367, we have 4 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0002, 2: -0.0002}.\n",
      "The model is updated in 0.001277923583984375 seconds\n",
      "At step 2, the loss is 0.5367, we have 3 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0002, 2: -0.0002}.\n",
      "The model is updated in 0.0012979507446289062 seconds\n",
      "At step 3, the loss is 0.5367, we have 2 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0002, 2: -0.0002}.\n",
      "The model is updated in 0.001237630844116211 seconds\n",
      "At step 4, the loss is 0.5367, we have 1 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0002, 2: -0.0002}.\n",
      "The model is updated in 0.0011870861053466797 seconds\n",
      "At step 5, the loss is 0.5367, we have 0 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0002, 2: -0.0002}.\n",
      "The model is updated in 0.0011539459228515625 seconds\n",
      "Returning the p={1: 0.3, 2: 0.7}, for which loss is 0.536658967412454\n",
      "At step 0, the loss is 0.4968, we have 5 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0, 2: -0.0}.\n",
      "The model is updated in 0.0012922286987304688 seconds\n",
      "At step 1, the loss is 0.4968, we have 4 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0, 2: -0.0}.\n",
      "The model is updated in 0.0013229846954345703 seconds\n",
      "At step 2, the loss is 0.4968, we have 3 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0, 2: -0.0}.\n",
      "The model is updated in 0.0014760494232177734 seconds\n",
      "At step 3, the loss is 0.4968, we have 2 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0, 2: -0.0}.\n",
      "The model is updated in 0.0013470649719238281 seconds\n",
      "At step 4, the loss is 0.4968, we have 1 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0, 2: -0.0}.\n",
      "The model is updated in 0.0012903213500976562 seconds\n",
      "At step 5, the loss is 0.4968, we have 0 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0, 2: -0.0}.\n",
      "The model is updated in 0.001280069351196289 seconds\n",
      "Returning the p={1: 0.3, 2: 0.7}, for which loss is 0.49681969304034745\n",
      "At step 0, the loss is 0.5587, we have 5 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0004, 2: -0.0004}.\n",
      "The model is updated in 0.0011959075927734375 seconds\n",
      "At step 1, the loss is 0.5587, we have 4 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0004, 2: -0.0004}.\n",
      "The model is updated in 0.0011501312255859375 seconds\n",
      "At step 2, the loss is 0.5587, we have 3 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0004, 2: -0.0004}.\n",
      "The model is updated in 0.001149892807006836 seconds\n",
      "At step 3, the loss is 0.5587, we have 2 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0004, 2: -0.0004}.\n",
      "The model is updated in 0.0011668205261230469 seconds\n",
      "At step 4, the loss is 0.5587, we have 1 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0004, 2: -0.0004}.\n",
      "The model is updated in 0.001155853271484375 seconds\n",
      "At step 5, the loss is 0.5587, we have 0 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: 0.0004, 2: -0.0004}.\n",
      "The model is updated in 0.001150369644165039 seconds\n",
      "Returning the p={1: 0.3, 2: 0.7}, for which loss is 0.5586799093615551\n",
      "At step 0, the loss is 0.5028, we have 5 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: -0.0007, 2: 0.0007}.\n",
      "The model is updated in 0.0011708736419677734 seconds\n",
      "At step 1, the loss is 0.5028, we have 4 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: -0.0007, 2: 0.0007}.\n",
      "The model is updated in 0.0011379718780517578 seconds\n",
      "At step 2, the loss is 0.5028, we have 3 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: -0.0007, 2: 0.0007}.\n",
      "The model is updated in 0.001148223876953125 seconds\n",
      "At step 3, the loss is 0.5028, we have 2 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: -0.0007, 2: 0.0007}.\n",
      "The model is updated in 0.0011227130889892578 seconds\n",
      "At step 4, the loss is 0.5028, we have 1 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: -0.0007, 2: 0.0007}.\n",
      "The model is updated in 0.0011188983917236328 seconds\n",
      "At step 5, the loss is 0.5028, we have 0 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: -0.0007, 2: 0.0007}.\n",
      "The model is updated in 0.0012698173522949219 seconds\n",
      "Returning the p={1: 0.3, 2: 0.7}, for which loss is 0.5028279972503013\n",
      "At step 0, the loss is 0.5428, we have 5 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: -0.0003, 2: 0.0003}.\n",
      "The model is updated in 0.001232147216796875 seconds\n",
      "At step 1, the loss is 0.5428, we have 4 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: -0.0003, 2: 0.0003}.\n",
      "The model is updated in 0.0012478828430175781 seconds\n",
      "At step 2, the loss is 0.5428, we have 3 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: -0.0003, 2: 0.0003}.\n",
      "The model is updated in 0.0012249946594238281 seconds\n",
      "At step 3, the loss is 0.5428, we have 2 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: -0.0003, 2: 0.0003}.\n",
      "The model is updated in 0.0012307167053222656 seconds\n",
      "At step 4, the loss is 0.5428, we have 1 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: -0.0003, 2: 0.0003}.\n",
      "The model is updated in 0.0013928413391113281 seconds\n",
      "At step 5, the loss is 0.5428, we have 0 patience left, and the probabilities are {1: 0.3, 2: 0.7}, which sum to 1.0000 with gradients {1: -0.0003, 2: 0.0003}.\n",
      "The model is updated in 0.0012960433959960938 seconds\n",
      "Returning the p={1: 0.3, 2: 0.7}, for which loss is 0.5428060198008495\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = run_simulation_with_consistent_group_labels()\n",
    "df = pd.DataFrame(results)\n",
    "df_melted = df.melt(var_name='Method', value_name='Accuracy')\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.boxplot(x='Method', y='Accuracy', data=df_melted)\n",
    "plt.title(\"Comparison of Test Accuracy Across Weighting Strategies\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
